---
title: "Analyse multidimensionnelle : MDS (Multidimensional Scaling)"
subtitle: "Cours de visualisation multidimensionnelle"
author: "Vadim L, Valentine J, Emilie N"
format:
  revealjs:
    theme: "serif"
    transition: fade
    slide-number: true
    center: true
    incremental: true
    mathjax: "default"
---

## Objectif du MDS (Multidimensional Scaling)

Méthode de représentation géométrique des données

**Objectif** : À partir de mesures de dissimilarité entre objets, on cherche à reconstruire une carte où les distances entre les points reflètent au mieux ces dissimilarités.

-   On peut partir de toute mesure de dissimilarité (pas forcément une distance métrique).

-   La carte reconstruite fournit des coordonnées $$x_i = (x_{i1}, x_{i2})$$ et la distance naturelle est : $$\|x_i - x_j\|_2 $$

Si deux objets sont très similaires → leurs points seront proches

Si deux objets sont très différents → leurs points seront éloignés

------------------------------------------------------------------------

## Famille de méthodes MDS

Le MDS est une famille d’algorithmes visant à trouver une configuration spatiale la plus optimal

Les principales méthodes :

-   **MDS classique (Classical MDS)**

-   **MDS métrique (Metric MDS)**

-   **MDS non métrique (Non-metric MDS)**

------------------------------------------------------------------------

## MDS classique : idée générale

Objectif : retrouver les coordonnées des points à partir d’une matrice de distances $D = (d_{ij})$ supposées euclidiennes.

$$
\|x_i - x_j\| \approx d_{ij}
$$

Plutôt que de calculer $X$ directement, on travaille avec la matrice de produits scalaires : $$
B = X X'
$$

Calcule de double centrage : $$
B = -\frac{1}{2} C D_2 C
$$ où $$C $$ est la matrice de centrage.

--\> On centre les distances autour du centre de gravité pour garantir une représentation équilibrée.

------------------------------------------------------------------------

## Étapes du calcul

Matrice des distances $D = (d_{ij})$

Matrice des carrés $D^2 = (d^2_{ij})$

Centrage de la matrice : $B = -\frac{1}{2} C D_2 C$

Décomposition en valeurs propres : $B = V \Lambda V'$

Coordonnées finales : $X^{(p)} = V_p \Lambda_p^{1/2}$

-   $\Lambda_p$ = sous-matrice $(p \times p)$ des plus grandes valeurs propres qui mesurent la variance sur chaque axe

-   $V_p$ = les colonnes associées de $V$ qui contient les vecteurs propres qui définissent les axes principaux

-   Les lignes de $X^p$ donnent les coordonnées des individus dans un espace p-dimensionnel (souvent petit avec p=2)

-   Les colonnes de $X^{(p)}$ sont les axes principaux de cet espace

On obtient une carte 2D/3D qui préserve au mieux les distances. En MDS classique celles-ci sont supposées euclidienne donc la reconstruction est exacte sinon on obtient la meilleure approximation.

------------------------------------------------------------------------

## Résumé : MDS classique

-   Conserve les distances euclidiennes autant que possible

-   Produit des axes principaux ordonnés selon leur importance qui maximisent la variance expliquées par les distances

-   Permet une visualisation claire des relations entre objets

-   Proche de l’ACP mais

    -   ACP part d'un tableau de variables –\> matrice de covariance

    -   MDS part d'une matrice de distance –\> coordonnées

------------------------------------------------------------------------

## Exemple : perception de différents types de fraises

4 types de fraises dégustées 2 fois par un jury composé de 12 personnes. Une note a été attibuée pour chaque descripteur sensoriel et pour chaque dégustation. On cherche à voir les différence et ressemblance entre les 4 types de fraises.

![](fraise plot.png){fig-align="center"}

------------------------------------------------------------------------

## Types de MDS

-   **MDS métrique** : les dissimilarités $d_{ij}$ sont quantitatives
-   **MDS non métrique** : les dissimilarités $d_{ij}$ sont ordinales ou qualitatives

Différence avec le MDS classique :\
Contrairement au cMDS, la mise à l’échelle des distances est un processus d’optimisation.\
On cherche à minimiser une **fonction de stress**, et la solution est obtenue par des **algorithmes itératifs**.

+:---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| \## Distance, dissimilarité et similarité                                                                                                                      |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Les notions de distance, dissimilarité et similarité (ou proximité) sont définies pour toute paire d’objets.                                                   |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| On peut se demander si les dissimilarités données sont **vraiment des distances**, et si elles peuvent être **interprétées comme des distances euclidiennes**. |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Étant donnée une matrice de dissimilarités \$ D = (d\_{ij}) \$, le MDS cherche à trouver des points \$ x_i,...x_n \$ appartenant à $\mathbb{R}^p$.             |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tels que : $$                                                                                                                                                  |
| d_{ij} \approx \|x_i - x_j\|_2                                                                                                                                 |
| $$                                                                                                                                                             |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Si la configuration exacte existe → distance euclidienne\                                                                                                      |
| Sinon → distance non euclidienne                                                                                                                               |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------+

## Exemple de distance non euclidienne

La distance radiale sur un cercle (longueur de l’arc entre deux points) :

Est bien une métrique,\
Mais ne peut pas être représentée exactement dans un espace euclidien $\mathbb{R}^p$.

Le MDS cherche une configuration approchée minimisant l’écart entre $d_{ij}$ et $\|x_i - x_j\|_2$.

## Photo exemple dans pdf FR

## MDS métrique

Le MDS métrique (ou *metric MDS*) cherche une configuration optimale $X \subset \mathbb{R}^p$ et une fonction monotone $f$ telles que : $$
f(d_{ij}) \approx \hat{d}_{ij} = \|x_i - x_j\|_2
$$

------------------------------------------------------------------------

## Fonction monotone et stress

$f$ peut être une fonction monotone paramétrique, par exemple : $$
f(d_{ij}) = \alpha + \beta d_{ij}
$$

La fonction de stress est : $$
\text{Stress} =
\left(
\frac{
\sum_{i<j} (\hat{d}_{ij} - f(d_{ij}))^2
}{
\sum d_{ij}^2
}
\right)^{1/2}
$$

Le MDS métrique minimise cette fonction sur $\hat{d}_{ij}, \alpha, \beta$.\
Le cas particulier $f(d_{ij}) = d_{ij}$ correspond au MDS classique.

------------------------------------------------------------------------

## Cartographie de Sammon (Sammon Mapping)

La **cartographie de Sammon** est une **généralisation du MDS métrique**.

La fonction de stress de Sammon est : $$
\text{Stress}_{\text{Sammon}} =
\frac{1}{\sum_{l<k} d_{lk}}
\sum_{i<j}
\frac{(\hat{d}_{ij} - d_{ij})^2}{d_{ij}}
$$

Les erreurs sont pondérées par $d_{ij}$, la distance d'origine, les petites distances on plus de poids ce qui : - préserve mieux les voisinages locaux, - donne une carte plus fidèle aux structures fines des données.

-   En conséquence, le Sammon mapping préserve les petits dij , leur accordant

    une plus grande importance dans la procédure d’ajustement que pour les valeurs plus élevées de dij

------------------------------------------------------------------------

## Comparaison : cMDS vs Sammon Mapping

+----------------+-------------+-----------------------------------+-------------------------------+
| Méthode        | Type        | Objectif                          | Avantage principal            |
+================+=============+===================================+===============================+
| cMDS           | Analytique  | Préserver distances euclidiennes  | Rapide, simple                |
+----------------+-------------+-----------------------------------+-------------------------------+
| Sammon Mapping | Numérique   | Pondérer les erreurs par distance | Meilleure préservation locale |
+----------------+-------------+-----------------------------------+-------------------------------+

------------------------------------------------------------------------

## MDS non métrique

Dans de nombreuses applications, les dissimilarités ne sont connues que par leur ordre de classement.

Le MDS non métrique cherche une configuration $X \subset \mathbb{R}^p$ telle que : $$
f(d_{ij}) \approx \hat{d}_{ij} = \|x_i - x_j\|_2
$$

On cherche uniquement à préserver l’ordre des dissimilarités : $$
d_{ij} < d_{kl} \iff f(d_{ij}) \le f(d_{kl}) \iff d_{ij}^* \le d_{kl}^*
$$

Les valeurs $d_{ij}^* = f(d_{ij})$ sont appelées **disparités**.\
Le MDS non métrique est donc basé sur les **rangs** plutôt que sur les valeurs absolues.

------------------------------------------------------------------------

## MDS non métrique de Kruskal

Kruskal (1964) propose de minimiser la fonction : $$
\text{Stress-1}(\hat{d}_{ij}, d_{ij}^*) =
\left(
\frac{
\sum_{i<j} (\hat{d}_{ij} - d_{ij}^*)^2
}{
\sum \hat{d}_{ij}^2
}
\right)^{1/2}
$$

Les dissimilarités initiales servent seulement à comparer les ordres :\
$d_{ij} < d_{kl} < ... < d_{mn}$

$f$ agit comme une régression monotone entre dissimilarités et distances.

------------------------------------------------------------------------

## Exemple : reconnaissance de lettres

Construction des dissimilarités à partir d’une matrice de similarité $\delta_{ij}$ : $$
d_{ij} =
\begin{cases}
c - \delta_{ij}, & \text{si } i \neq j \\
0, & \text{si } i = j
\end{cases}
$$ avec $c \ge \max(\delta_{ij})$.

------------------------------------------------------------------------

## Choix de la méthode

Comme les dissimilarités $d_{ij}$ dépendent du choix arbitraire de $c$,\
le MDS non métrique est souvent plus logique ici.\
Cependant, les méthodes métriques (cMDS, Sammon) peuvent aussi donner de bons résultats.
